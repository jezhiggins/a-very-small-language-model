= A Very Small Language Model

== Hi, how's everyone

Long day, eh? Well, if you fancy more hard thinking there's Dietmar to my right doing Async {cpp}, Robert on my left looking at Monadic Operations in {cpp} (you'll all recall, no doubt that "All told, a monad in X is just a monoid in the category of endofunctors of X, with the cartesian product of sets replaced by composition of endofunctors and unit set by the identity endofunctor.") and round the corner we've got Jutta and Willem doing a couple of shorter talks on development processes. It's fairly hefty competition, and I'm very flattered you've chosen to be here. This session is, by some measure probably the lightest of the day (even the week), and I hope we can have some fun with some code, and just all have a nice time. If, however, having a nice time doesn't appeal, and you think maybe you'd rather skip out to one of the other sessions, I shan't be the least bit offended, so please do vote with you feet if you get the urge.

== So what are we going to do?

We're going to build a bit of magic software in the hour or so.

== Magic software

What do I mean by magic software?

There are lots of things in software that seem kind of amazing, even to people who work in software. Operating systems, compilers & interpreters, windowing systems - things we use all day every day - are obvious choices. Sat navs! Lots of game stuff - I'm not just talking about graphics rendering, but things like in-game AI. Satnavs, they're pretty cool.

Does anyone here play Dominion? There's a lovely version of Dominion for your tablet, and the computer opponents are not just really good, but will play the same cards in different ways. It doesn't just come up with one optimised path, it'll do different things, sometimes in response to what you do and sometimes because, I don't know, it just feels like it.

Even the more mundane stuff can be fascinating - there was an incredible article about Hans Boehm's calculator app. It magical because it can do (10^100) + 1 − (10^100) correctlyfootnote:[https://chadnauseam.com/coding/random/calculator-app] through the miracle of quite a lot of hard maths.

These are, however, all pretty large topics. You can’t write an operating system, or even a calculator, in  a 90 minute session. I can’t anyway. I am not Ken Thompson.

== Do you want AI?

So, about this time last year, I was poking around on my phone doing something or other, and my keyboard suddenly asked me if I want to turn on some kind of AI assistant to “boost productivity” and “unlock creativity”.

Obviously, I said no, because honestly, how much productivity boosting and creativity unlocking do you to send WhatsApp messages? And, in any case, it can’t do that.

However, it did get me thinking. It did! Really - I'm not giving you some kind of trite homily here - I promise you it did.

We are all, of course, familiar with how phone keyboards in particular, and more widely the general act of typing things on a computer, offer up suggestions for the word you're currently typing, and also the next word you might want to type.

The first time I encountered next word prediction was in late 1991 or early 1992, in a piece of education software. It was, for all intents and purposes, a word processor to help children with specific learning difficulties (dyspraxia, dyslexia, or whatever) write more easily. As you typed, it popped up a selection of suggested words and you could just pick them off the list. Commonplace these days, but quite startling if you've never seen it before.

Now, here was a keyboard rebranding “next word prediction” as “AI assistance”. It's attempting to groups this technology in the same bucket as the ChatGPTs and the Claudes and what have you. It's trying to stoke up the idea that text prediction is a hard problem, something best left to the big brains at Google or Apple or Microsoft with their shiny offices and free lunches in Silicon Valley and Redmond.

But, hang on, if a BBC Master system could do this 35 years ago, how hard could it be?

Can we build one in an hour?

== Spoiler

I originally thought 45 minutes, because I think this would make a cool session at a different conference with shorter slots, so I was reallly going for it. But I was also watching a really good FIH ProLeague hockey match at the same time, so it took about 80 minutes for the first cut.

Next ProLeague hockey is the 7 June - Spain-Argentina double header, then Netherlands-Australia women from the Wagener in Amstelveen. Should all be cracking. I'm predicting Spain women, Spain men, Netherlands women (never bet against the Dutch women) btw. Point is, no such distractions today - only distractions of my own making.

== A quick note about implementation language

My first language is {cpp} (it's not, by you know what I mean), and perhaps because of that, I’ve had conversations with people who work in C# or  Python or PHP or similar say that all that low-level stuff is too complicated for them. I’ve also encountered a lot of {cpp} (and C and assembler) snobs who look down the noses at languages you can’t cause a segfault with and the people who use them.

The latter are probably beyond redemption, but it kind of boils my piss that there are programmers who’ve taken some of that snobbery onto themselves and accept that it’s founded in any kind of truth. C++ programmers aren’t on some rarefied mountain top. Programming’s programming. Software’s the most malleable medium we could wish to work in. Anybody can do anything in software.

So, I'm doing this in JavaScript :D  If we can do magic stuff in JavaScript, then we can do anything.

Besides, it fits on slides better

Remember, you can switch to another talk any time you like.

I've used the phrase "magic software". I don't necessarily mean it's particularly unknown, or requires deep knowledge, or anything like that. Writing an operating system is pretty well described and has been for decades. Sometimes all you need a half a clue and to bash away it for a while.

== That's the preamble

Shall we get started?

== Markov chains

The half a clue I had was the name "Markov chain". I knew you that was something to do with it, so I messages by son Daniel, who's doing a PhD in Statistics and said

"Remind me what a Markov Chain is"

"Dad, that's GCSE stuff"

"So remind me. I didn't do GCSEs"

"Fossil"

I did send him another message, but the last time I said a naughty word during a talk the video didn't get published.

== What Is A Markov Chain?

> A Markov chain is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

* _stochastic process_ - this is mathematician talk for _random process_
* _sequence of events_ - we can keep going
* _probability of each event depends only on the state attained in the previous event_ -  what happens next depends only on where we are now, not on how we got here

* it's called a Markov chain for Andrey Markov, who was a Russian mathematician who did a lot of work on stochastic process around the turn of the 20th Century - 1856 to 1922

<picture>

The sum of the probabilities of the new states is always 1.

== How Does It Apply To Text Prediction?

What if, instead of state A I had "hello", instead of state B I had "world", and instead of state C I had "there"

Start at "hello"

Pick a number

Move to "world"

Try again, start at "hello"

Pick a number

Move to "world"

Try again, start at "hello"

Pick a number

Move to "there"

I mean, it's not nothing, but you get the idea.

== Building the chain

To build the model, to drive the chain, we need first need a body of text - a corpus, as it is called in the text processing trade.

Now, formally, a text corpus is large and systematic. For example the Corpus of London Teenage Speech is a set of samples of spoken English, collected in 1993 from recorded and transcribed conversations by teenagers between the ages of 13 and 17. It's about half a million words, and contains 67 examples of the word 'bum'.

For my purposes I used a corpus I had to hand, which was a download of all my Mastodon toots.

== Training the chain

We have our corpus. How do we use it to train our chain?

Well, it's really quite simple. We just look at each word in turn, and see what comes after it. Now some following words will occur more than once, so we keep a running count of those occurrences. Once we've read our whole corpus, we will have a list of all the words, and which words come after them.

<simple example walk though>

== Generate some toots

== Austenesque

== Sherlockian

== JavaScript-a-go-go

== Sliding windows

one token, vs two tokens, etc

== As you type

== These *are* language models

Very small language models, very simple language models, but nonetheless, they capture some essentiall essence of the source corpuses (corpi? no).

=== How could we make them "better" models

=== Stemming

=== Parts of speech tagging

nouns, verbs, adverbs, etc

Controlled languages - noun-phrases

=== Vector representation of words

These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. Word2vec was developed by Tomáš Mikolov and colleagues at Google and published in 2013.

=== No time

These represent rather more work that my "can I do this in the time it takes to play a game of hockey", but NLP is a fascinating subject and if your interest is piqued, do investigate.

== A Horrific Chimera

If we had some nice Austen, and some ok Holmes, and some, well, some JavaScript, if we put them all together, it'll be even better right?

But, no

== Even small systems get the blues

On the one hand, this is a just a bit of fun. Programming for money is great and all, but it's often quite run of the mill, so if you need to, just bunk off for a couple of hours - noone will notice - and write something fun, something a bit magic. You'll feel better for it.

On the other hand, the title of this talk is obviously an allusion to the AI hype that's swirling and whirling all around us. Even a tiny system like this can display surprising, and convincing, behaviour, but as we feed it more and more text, it actually gets worse and worse. Bigger systems will display even more surprising and convincing behaviour, but as they grab more and more text, it looks like they too will actually get worse and worse.

I've just said programming can be a ton of fun, because it can. We can reach out and explore these incredible problem spaces, and for each problem there's an infinity of solutions. There's this incredible multidimensional space we can shape and travel through, so it can be fun, but it can be dangerous. There are consequences to the things we build too. So, have fun, but let's be careful out there.

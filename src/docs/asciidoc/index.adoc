= A Very Small Language Model
Jez Higgins, jez@jezuk.co.uk
{docdate}
:notitle:
:customcss: style/theme-tweak.css
:revealjs_theme: night
:revealjs_totalTime: 4800
:revealjs_progress: true
:revealjs_controls: true
:revealjs_transition: none
:revealjs_slideNumber: c/t
:revealjs_showSlideNumber: speaker

== !

[big]*A Very Small Language Model*

[NOTE.speaker]
--
Hello friends, I'm Jez and this session is the lightest of the day (perhaps even of the week).

It's been a long day, eh? Well, if you fancy more hard thinking there's Dietmar to my right doing Async {cpp}, Robert on my left looking at Monadic Operations in {cpp} (you'll all recall, no doubt that _All told, a monad in X is just a monoid in the category of endofunctors of X, with the cartesian product of sets replaced by composition of endofunctors and unit set by the identity endofunctor_) and round the corner we've got Jutta and Willem doing a couple of shorter talks on development processes.

It's fairly hefty competition, and I'm very flattered you've chosen to be here. This session is, by some measure probably the lightest of the day (even the week), and I hope we can have some fun with some code, and relax a bit before the lightning talks. If, however, having a nice time doesn't appeal, and you think maybe you'd rather skip out to one of the other sessions, I shan't be the least bit offended, so please do vote with you feet if you get the urge.

So, what are we going to do?

We're going to build a bit of magic software in an hour or so.
--

== !

[big]*Magic Software*

[NOTE.speaker]
--
What do I mean by magic software?

There are lots of things in software that seem kind of amazing, even to people who work in software. Operating systems, compilers & interpreters, windowing systems - things we use all day every day - are obvious choices.

I mean, does anyone really understand operating systems? We passed the comprehension limit (which I'm going to confidently state is about 10 thousand lines but without the reference to back it up) for operating systems with 6th Edition Unix. It's incredible they work at all!

Perhaps operating systems are less a source of wonder, and more a source of irritation to the wider population?

Sat navs! They're pretty cool, although obviously they need a lot of specialist hardware, and a whole load of mapping data.
--

== !

cyclestreets image

[NOTE.speaker]
--
Here's a satnav in action - it's getting real time GPS updates, so it's able to show position, heading, and speed (to within a couple of metres, so Heisenberg bugs are not an issue). We've got a map data, so we know how far to the next turning and which way we need to turn, how much of the journey distance is left, how much of the journey we've already covered and how quickly so it can calculate how long it'll take to arrive.

Lots of games stuff is pretty amazing - I'm not just talking about graphics rendering, but things like in-game AI.
--

== !

image::images/Dominion.jpg[]

[NOTE.speaker]
--
This is the start of a game of Dominion. It's a deck builder game, so you start with a handful of cards, and as the game progresses you gain more cards from the middle here.

Each of these action cards has a different effect, these over here can be used to modify the behaviour of them. The aim is get the most of these green cards over here, but you don't want to go to early, because they're otherwise useless and take up space in your hand here.

There's a lot to balance.  And every game is almost guaranteed to be unique - there are hundreds of these different action cards, and so you might need a very different approach from one game to the next.

The computer opponents are really good, particularly because they will play the same cards in different ways. It doesn't just come up with one optimised path, it'll do different things, sometimes in response to what you do and sometimes because, I don't know, it just feels like it.

It's terrific. I have no idea how it does it, it's just great.
--

== !

image::images/calculator.png[]

[NOTE.speaker]
--
Even the more mundane stuff can be fascinating, once you know about it. There's  an incredible article about Hans Boehm's calculator app. It magical because it can do (10^100) + 1 − (10^100) correctly through the miracle of quite a lot of hard maths.

These are, however, all pretty large topics. You can’t write an operating system, or even a calculator, in  a 90 minute session. I can’t anyway. I am not Ken Thompson.
--

== !

image::images/hello-keyboard.png[]

[NOTE.speaker]
--
So, about this time last year, I was poking around on my phone doing something or other, and my keyboard suddenly asked me if I want to turn on some kind of AI assistant to “boost productivity” and “unlock creativity”.

Obviously, I said no, because honestly, how much productivity boosting and creativity unlocking do you to send WhatsApp messages? And, in any case, it can’t do that.

However, it did get me thinking. It did! Really - I'm not giving you some kind of trite homily here - I promise you it did.

We are all, of course, familiar with how phone keyboards in particular, and more widely the general act of typing things on a computer, offer up suggestions for the word you're currently typing, and also the next word you might want to type.

The first time I encountered next word prediction was in late 1991 or early 1992, in a piece of schools' software. It was, for all intents and purposes, a word processor, and it was intended to help children with specific learning difficulties (dyspraxia, dysgraphia, dyslexia, or whatever) write more easily. As you typed, it popped up a selection of suggested words and you could just pick them off the list. I think it would also suggest next words. Commonplace these days, but really quite startling, even slightly spooky and magical, if you've never seen it before.

Now, here was a keyboard rebranding “next word prediction” as “AI assistance”. This is an attempt to group this technology in the same bucket as the ChatGPTs and the Claudes and what have you. It's trying to stoke up the idea that text prediction is a hard problem, something best left to the big brains at Google or Apple or Microsoft with their shiny offices and free lunches in Silicon Valley and Redmond.

Now, I don't know about you, and with respect to Anastasia, I think our general experience of the AI chatbots has not been magical. Quite the contrary.

These gys (and you know it's guys) are underselling text prediction :)
--

== !

image::images/Acorn_BBC_Master_Series_Transparent.png[]

[.text-right]
--
[.small]#Photo courtesy of Dejdżer / Digga, \https://commons.wikimedia.org/wiki/File\:Acorn_BBC_Master_Series.jpg#
--

[NOTE.speaker]
--
So, I thought, if a BBC Master system could do this 35 years ago, how hard could it be?

Could I recreate the magic?
--

== !

[.big]#In an hour?#

[NOTE.speaker]
--

I'm not quite sure where this came from, but I'd just sat down to watch  a terrifically good FIH ProLeague hockey match at the same time. International hockey matches are 60 minutes long, so maybe that was it.

Next ProLeague hockey is the 7 June btw - Spain-Argentina double header, then Netherlands-Australia women from the Wagener in Amstelveen. Should all be cracking. I'm predicting Spain women, Spain men, Netherlands women (never bet against the Dutch women) btw. Point is, no such distractions today - only distractions of my own making.
--

== !

[.big]#Spoiler: YES#

[NOTE.speaker]
--
Before we find out, I'd just like to let you know that all the code we'll be looking at today will be in JavaScript.

My first language is {cpp} (it's not, by you know what I mean), and perhaps because of that, I’ve had conversations with people who work in C# or Python or PHP or similar and they'll say something like _all that low-level stuff is too complicated for them_. Is that familiar to anyone here? I’ve also encountered a bit of the reverse - {cpp} (and C and assembler) snobs who look down the noses at languages you can’t cause a segfault with and, by extension, the people who use them.

The latter are probably beyond redemption, but it kind of boils my piss that there are programmers who’ve taken some of that snobbery onto themselves and accept that it’s founded in any kind of truth. {cpp} programmers aren’t on some rarefied mountain top. Programming’s programming. Software’s the most malleable medium we could wish to work in. Anybody can do anything in software.

So, I'm doing this in JavaScript :D  If we can do magic stuff in JavaScript, then we can do anything.

Besides, it fits on slides better
--

== Remember

[.big]#You can switch to another talk any time you like#

[NOTE.speaker]
--
That's the preamble. Shall we get started?

I've used the phrase "magic software" a few times now. I don't necessarily mean it's particularly unknown, or requires deep knowledge, or anything like that. Writing an operating system is pretty well described and has been for decades. Sometimes all you need a half a clue and to bash away it for a while.
--

== !

The Half A Clue

[.big]#Markov Chains#

[NOTE.speaker]
--
The half a clue I had was the name "Markov chain". I knew you that was something to do with it, and kind of vaguely knew it was something to do with probability.

So I messaged by son Daniel, who's got a degree in Maths and Statistics, and is doing a PhD in Stochastic Epidemiology -
--

== !

image::images/whats-a-markov-chain.jpeg[]

[NOTE.speaker]
--
Classic Dad behaviour - pretending I know but it's just slipped my mind.
--

== !

image::images/gcse-stuff.jpeg[]

[NOTE.speaker]
--
Hmm
--

== !

image::images/again.jpeg[]

[NOTE.speaker]
--
Ok, so he just needs a bit of chivvying along
--

== !

image::images/fossil.jpeg[]

[NOTE.speaker]
--
So it was at this point I turned to other sources.

I did send him another message, but the last time I said a naughty word during a talk the video didn't get published.
--

== !

[quote]
--
A Markov chain is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.
--

[NOTE.speaker]
--
A random process in which the transition probability only depends on where you are right now (i.e. the future is independent of the past).

* _stochastic process_ - this is mathematician talk for _random process_
* _sequence of events_ - we can keep going
* _probability of each event depends only on the state attained in the previous event_ -  what happens next depends only on where we are now, not on how we got here, the future is independent of the past

* it's called a Markov chain for Andrey Markov, who was a Russian mathematician who did a lot of work on stochastic processes around the turn of the 20th Century - 1856 to 1922.

--

== !

[plantuml, state-diagram-1, svg, width=600]
----
object A
object B
object C
A -r-> B : 0.4
A --> C : 0.6
B --> A : 0.7
B --> C : 0.3
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
----

[NOTE.speaker]
--
Here's a little Markov process.

If we start over here in State A, then the probability of us moving to state B is 0.4, and the probability of moving to state C is 0.6. So, B or C? Let's hear it for B! Let's hear it for C!

Note that the sum of the state transition probabilities is always 1. Even if we can stay where we are, that's still a transition, still gets its own probability.

We're all programmers, the probability that we didn't even
notice that this is a discrete-time Markov chain approaches 1. You can build continuous time Markov processes, but I've no idea how you'd actually do that, and it probably involves some kind of fearsome calculus, so let's pretend I never even mentioned it.

So, we have this little state space, that we're randomly and memorylessly mocing around. These have, perhaps surprisingly, a number of real world applications

* Including in macroeconomics - stock price equilibrium, for example, and various Monte Carlo simulations
* Markov chains can be used to model interaction between state actors. We may have to reevaluate that in the light of recent events, but it's been done.
* They have application in algorithmic music generation
* Baseball analytics, apparently (and perhaps not real world)

And, of course, since you've all read the synopsis of this session, you can use Markov chains for algorithmic text generation
--

== !

[.big]#Algorithmic text generation#

[NOTE.speaker]
--
How do we apply Markov chains to text generation?
--

== !

[plantuml, state-diagram-2, svg, width=600]
----
object A
object B
object C
A -r-> B : 0.4
A --> C : 0.6
B --> A : 0.7
B --> C : 0.3
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
----
== !


[plantuml, state-diagram-3, svg, width=600]
----
object A
A : hello
object B
object C
A -r-> B : 0.4
A --> C : 0.6
B --> A : 0.7
B --> C : 0.3
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
----

[NOTE.speaker]
--
Well, what if instead of state A was associated with "hello"
--


== !

[plantuml, state-diagram-4, svg, width=600]
----
object A
A : hello
object B
B : there
object C
A -r-> B : 0.4
A --> C : 0.6
B --> A : 0.7
B --> C : 0.3
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
----

[NOTE.speaker]
--
Instead of state B we had "there"
--


== !

[plantuml, state-diagram-5, svg, width=600]
----
object A
A : hello
object B
B : there
object C
C : world
A -r-> B : 0.4
A --> C : 0.6
B --> A : 0.7
B --> C : 0.3
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
----


[NOTE.speaker]
--
And instead of state C, we had, of course, the Brian Kernighan approved word "world"

So, if we start at "hello", we might transition to, aka generate the next word, "there", or we might generate "world".

Start at "hello". Pick a number. Transition to, aka generate the next world, "world".

Try again, start at "hello". Pick a number. Move to "world"

Try again, start at "hello". Pick a number. Move to "there"

And lo, we algorithmic text generation. I mean, it's not huge, but it's not nothing.

*So that's it right? End of talk.*

I'd buy you all a drink but I'm not sure I can afford it at Bristol prices.

What's the missing step here?
--

== !

[.big]#How do we know what the probabilities are?#

[NOTE.speaker]
--
How do we know what the probabilities are?

Also, even before that, how do we know what the words are?
--

== !

[.big]#A model#

[NOTE.speaker]
--
What we need is a model, a model of, in our case, English as she is spoke.

And to make a model, we need something to make a model of.

We need some text, or a corpus, as it's known in the natural language processing trade.
--

== !

[.big]#a Text Corpus#

[NOTE.speaker]
--
Now, in more formal terms, a text corpuses large and systematic - they're aiming to capture something particular.

For example the Corpus of London Teenage Speech is a set of samples of spoken English, collected in 1993 from recorded and transcribed conversations by teenagers between the ages of 13 and 17. It's about half a million words, and contains 67 examples of the word 'bum'.

For my purposes I used a text corpus I had to hand
--

== !

image::images/toots.png[]

[NOTE.speaker]
--
My Mastodon archive.
--

[background-iframe="files/toots.html"]
== !


[NOTE.speaker]
--
Here it is. 2384 scintillating lines, containing 46576 words.

I've cleaned it up a little to take out other people's handles, and, erm, remove the stronger swearwords -

and looking it now, it already looks like it was generated by some kind of stochastic process. Hmm.

So, how do we build a model from this?
--

== !

[.small]#The last thing I did today was delete a file called 'burp.md'#

I don't think this is quite as reassuring as you think it is 😉

[.big]#Is this the code phrase to activate a sleeper agent?#

1 or infinity, yea. Those are the only two numbers that matter.

[.small]#As long as there's plenty of space to clop around on your horse, who really cares?#

[NOTE.speaker]
--
We're going to take each line in turn, and split it into pairs of words.

More precisely pairs of tokens.
--


== !

[.big]#pass:[<u>Is this</u>] the code phrase to activate a sleeper agent?#

== !

[.big]#Is pass:[<u>this the</u>] code phrase to activate a sleeper agent?#

== !

[.big]#Is this pass:[<u>the code</u>] phrase to activate a sleeper agent?#

== !

[.big]#Is this the pass:[<u>code phrase</u>] to activate a sleeper agent?#

== !

[.big]#Is this the code pass:[<u>phrase to</u>] activate a sleeper agent?#

== !

[.big]#Is this the code phrase pass:[<u>to activate</u>] a sleeper agent?#

== !

[.big]#Is this the code phrase to pass:[<u>activate a</u>] sleeper agent?#

== !

[.big]#Is this the code phrase to activate pass:[<u>a sleeper</u>] agent?#

== !

[.big]#Is this the code phrase to activate a pass:[<u>sleeper agent</u>]?#

[NOTE.speaker]
--
What should we do now? Include the question mark, or not include the question mark? As part of the work, or as a token of its own?

Up to you. I chose to have it as a token of its own
--

== !

[.big]#Is this the code phrase to activate a sleeper pass:[<u>agent ?</u>]#

[NOTE.speaker]
--
Often we think of parsing as quite a sophisticated task. Not so here. It's slightly more sophisticated than splitting on spaces, although not much more.
--

== !

[source, javascript]
----
function* tokenise(input) {
  const stream = streamer(input);

  let c = stream.next();
  while (stream.hasNext()) {
    while (isWhitespace(c))
      c = stream.next();

    let token = "";
    for ( ; isAlphanumeric(c); c = stream.next())
      token += c;

    if (token)
      yield token;

    for (; isOtherCharacter(c); c = stream.next())
      yield c;
  }
}
----

[NOTE.speaker]
--
Here we are - this is the tokeniser I put together.

For those of you not familiar, this `function*` means this is a JavaScript generator. If you know Python generators, it's the same but with more curly brackets. It's akin to coroutine - you can yielf values, then continue on from there.

`streamer` just puts a little `next`/`hasNext` face on the input, so we can loop over it more easi

We start by discarding whitespace, then gather up anything that looks like it could be part of a word and yield that, then if's any punctuation yield that, and round and round until we run out, yielding up a token at a time.

If we put another little layer on top, we can return pairs of words ...
--

=== !

[source, javascript]
----
function streamer(input) {
  let index = 0;
  return {
    next: () => (index !== input.length ? input[index++] : null),
    hasNext: () => index !== input.length,
  };
}
----

== !

[source, javascript]
----
async function* wordPairs(filename) {
  const file = await open(filename);

  let prev = null;
  for await (const line of file.readLines()) {
    for (const token of tokenise(line)) {
      yield [prev, token];
      prev = token;
    }

    yield [prev, null];
  }
}
----

[NOTE.speaker]
--
Does this async/await trouble anyone?

So we've got our `tokenise` generator splitting out toots into tokens, then we've layered another generator around it to pair up those tokens.

We are, essentially running a sliding window over our input stream.

So, we have our pairs of tokens - one at the front, one at the back - are we're just going to build a big list of the tokens in the front, with whatever comes after them and, crucially, how many times each of the following tokens occurs
--

== !

[code]
----
is      ->  (a, 59)
            (the, 24)
            (',', 20)
            (that, 17)
            ... 207 other tokens ...
            (rarely, 1)

grateful -> (to, 1)
            (!, 1)
            (nation, 1)

...
----

[NOTE.speaker]
--
And so on, for another 8379 tokens.  8379 seems like a lot, but I can only assume there are a lot of a) names and b) horrific misspellings in there.

All this is clear, yes? We can read our source corpus, generating a series of token pairs, producing a big list of each unique token we encounter, together with the counts of the tokens that follow.

And this is our language model.
--

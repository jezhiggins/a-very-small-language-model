= A Very Small Language Model
Jez Higgins, jez@jezuk.co.uk
{docdate}
:notitle:
:customcss: style/theme-tweak.css
:revealjs_theme: night
:revealjs_totalTime: 4800
:revealjs_progress: false
:revealjs_controls: false
:revealjs_transition: none
:revealjs_slideNumber: c/t
:revealjs_showSlideNumber: speaker

[background-image="images/title-card.png"]
== !

[NOTE.speaker]
--
Hello friends, I'm Jez.

It's been a long day, eh? Well, if you fancy more hard thinking there's Dietmar to my right doing Async {cpp}, Robert on my left looking at Monadic Operations in {cpp} (you'll all recall, no doubt that _All told, a monad in X is just a monoid in the category of endofunctors of X, with the cartesian product of sets replaced by composition of endofunctors and unit set by the identity endofunctor_) and round the corner we've got Jutta and Willem doing a couple of shorter talks on development processes.

It's fairly hefty competition, and I'm very flattered you've chosen to be here. This session is, by some measure probably the lightest of the day (even the week), and I hope we can have some fun with some code, and relax a bit before the lightning talks. If, however, having a nice time doesn't appeal, and you think maybe you'd rather skip out to one of the other sessions, I shan't be the least bit offended, so please do vote with you feet if you get the urge.

So, what are we going to do?

We're going to talk about a bit of magic software
--

== !

[big]*Magic Software*

[NOTE.speaker]
--
There are lots of things in software that seem kind of amazing, even to people who work in software. Operating systems, compilers & interpreters, windowing systems - things we use all day every day - are obvious choices.

I mean, does anyone really understand operating systems? We passed the comprehension limit (which I'm going to confidently state is about 10 thousand lines but without the reference to back it up) for operating systems with 6th Edition Unix, and that was in 1975. There are device drivers bigger than that now. Operating systems are so big, it's incredible they work at all!

Perhaps operating systems are less a source of wonder, and more a source of irritation to the wider population?

Sat navs! They're pretty cool, although obviously they need a lot of specialist hardware, and a whole load of mapping data.
--

== !

image::images/cyclestreets.jpeg[]

[NOTE.speaker]
--
Sat navs! They're pretty cool, although obviously they need a lot of specialist hardware, and a whole load of mapping data.

Here's a satnav in action - it's getting real time GPS updates, so it's able to show position, heading, and speed (to within a couple of metres, so Heisenberg bugs are not an issue). We've got a map data, so we know how far to the next turning and which way we need to turn, how much of the journey distance is left, how much of the journey we've already covered and how quickly so it can calculate how long it'll take to arrive.

They're pretty cool.

Lots of games stuff is pretty amazing - I'm not just talking about graphics rendering, but things like in-game opponents
--

== !

image::images/Dominion.jpg[]

[NOTE.speaker]
--
This is the start of a game of Dominion. It's a deck builder game, so you start with a handful of cards, and as the game progresses you gain more cards from the middle here.

Each of these action cards has a different effect, these over here can be used to modify the behaviour of them. The aim is get the most of these green cards over here, but you don't want to go to early, because they're otherwise useless and take up space in your hand here.

There's a lot to balance.  And every game is almost guaranteed to be unique - there are hundreds of these different action cards, and so you might need a very different approach from one game to the next.

The computer opponents are really good, particularly because they will play the same cards in different ways. It doesn't just come up with one optimised path, it'll do different things, sometimes in response to what you do and sometimes because, I don't know, it just feels like it.

It's terrific. I have no idea how it does it, it's just great.
--

== !

image::images/calculator.png[]

[NOTE.speaker]
--
Even the more mundane stuff can be fascinating, once you know about it. There's  an incredible article about Hans Boehm's calculator app. It magical because it can do (10^100) + 1 − (10^100) correctly through the miracle of quite a lot of hard maths.

These are, however, all pretty large topics. You can’t cover an operating system, or even a calculator, in the time available to us today. I can’t anyway. I am not Ken Thompson.
--

== !

image::images/hello-keyboard.png[]

[NOTE.speaker]
--
So, about this time last year, I was poking around on my phone doing something or other, and my keyboard suddenly asked me if I want to turn on some kind of AI assistant to “boost productivity” and “unlock creativity”.

Obviously, I said no, because honestly, how much productivity boosting and creativity unlocking do you to send WhatsApp messages? And, in any case, it can’t do that.

We are all, of course, familiar with how phone keyboards in particular, and more widely the general act of typing things on a computer, first of all act as spelling correction on the word you're typing, but also to prompt you with the next word _you might want to type_.

You might have occasionally played that game where you just accept the suggestions it gives you ...
--

== !

image::images/just-accept.png[]

[NOTE.speaker]
--
The first time I encountered next word prediction was in late 1991 or early 1992, in a piece of schools' software. It was, for all intents and purposes, a word processor, and it was intended to help children with specific learning difficulties (dyspraxia, dysgraphia, dyslexia, or whatever) write more easily. As you typed, it popped up a selection of suggested words and you could just pick them off the list. It could also suggest next words.

Commonplace these days, but really quite startling, even slightly spooky and magical, if you've never seen it before.

Now, here was a keyboard rebranding “next word prediction” as “AI assistance”. This is an attempt to group this technology in the same bucket as the ChatGPTs and the Claudes and what have you. It's trying to stoke up the idea that text prediction is a hard problem, something best left to the big brains at Google or Apple or Microsoft with their shiny offices and free lunches in Silicon Valley and Redmond.

Now, I don't know about you, and with respect to Anastasia, I think our general experience of the AI chatbots has not been magical. Quite the contrary.

These guys (and you know it's guys) are overselling text prediction :)
--

== !

image::images/Acorn_BBC_Master_Series_Transparent.png[]

[.text-right]
--
[.small]#Photo courtesy of Dejdżer / Digga, \https://commons.wikimedia.org/wiki/File\:Acorn_BBC_Master_Series.jpg#
--

[NOTE.speaker]
--
So, I thought, if a BBC Master system could do this 35 years ago, how hard could it be?

Could I recreate the magic?
--

== !

[.big]#In an hour?#

[NOTE.speaker]
--

I'm not quite sure where this came from, but I'd just sat down to watch a FIH ProLeague hockey match at the same time. Can't remember which one - there was a lot of hockey last February, but I remember it was really good. Possibly it was India vs Spain, which went to 8-7 in shuffles after a 2-2 draw. Anyway. International hockey matches are 60 minutes long, so maybe that was it.

Next ProLeague hockey is the 7 June btw - when there is frankly too much on. Spain-Argentina double header from Valencia, then Netherlands-Australia women and Netherlands-India men from the Wagener in Amstelveen. Should all be amazing. I'm predicting Spain women, Spain men, Netherlands women (never bet against the Dutch women), and, oh, India. Point is, no such distractions today - only distractions of my own making.
--

== !

[.big]#Spoiler: YES#

[NOTE.speaker]
--
Before we go on, I'd just like to let you know that all the code we'll be looking at today will be in JavaScript.

My first language is {cpp} (it's not, by you know what I mean), and perhaps because of that, I’ve had conversations with people who work in C# or Python or PHP or similar and they'll say something like _all that low-level stuff is too complicated for them_. Is that familiar to anyone here? I’ve also encountered a bit of the reverse - {cpp} (and C and assembler) snobs who look down the noses at languages you can’t cause a segfault with and, by extension, the people who use them.

The latter are probably beyond redemption, but it kind of infuriates me that there are programmers who’ve taken some of that snobbery onto themselves and accept that it’s founded in any kind of truth. {cpp} programmers aren’t on some rarefied mountain top. Programming’s programming. Software’s the most malleable medium we could wish to work in. Anybody can do anything in software.

So, I'm doing this in JavaScript :D  If we can do magic stuff in JavaScript, then we can do anything.

Besides, it fits on slides better
--

== Remember

[.big]#You can switch to another talk any time you like#

[NOTE.speaker]
--
That's the preamble. Shall we get started?

I've used the phrase "magic software" a few times now. I don't necessarily mean it's particularly unknown, or requires deep knowledge, or anything like that. Writing an operating system is pretty well described and has been for decades.

What I mean is the effect is magical, in some way

To implement it, sometimes all you need a half a clue and to bash away it for a while.
--

== !

The Half A Clue

[.big]#Markov Chain#

[NOTE.speaker]
--
The half a clue I had was the name "Markov chain". I knew that was something to do with it, and kind of vaguely knew it was something to do with probability.

So I messaged by son Daniel, who's got a degree in Maths and Statistics, and is doing a PhD in Stochastic Epidemiology -
--

== !

image::images/whats-a-markov-chain.jpeg[]

[NOTE.speaker]
--
Classic Dad behaviour - pretending I know but it's just slipped my mind.
--

== !

image::images/gcse-stuff.jpeg[]

[NOTE.speaker]
--
Hmm
--

== !

image::images/again.jpeg[]

[NOTE.speaker]
--
Ok, so he just needs a bit of chivvying along
--

== !

image::images/fossil.jpeg[]

[NOTE.speaker]
--
So it was at this point I turned to other sources.

I did send him another message, but the last time I said a naughty word during a talk the video didn't get published.
--

== !

[quote]
--
A Markov chain is a stochastic process +
describing a sequence of possible events +
in which the probability of each event +
depends only on the state attained in the previous event.
--

[NOTE.speaker]
--
A Markov chain is a stochastic process +
describing a sequence of possible events +
in which the probability of each event +
depends only on the state attained in the previous event.

* _stochastic process_ - this is mathematician talk for _random process_
* _sequence of events_ - we can keep going
* _probability of each event depends only on the state attained in the previous event_ -  what happens next depends only on where we are now, not on how we got here, the future is independent of the past

* it's called a Markov chain for Andrey Markov, who was a Russian mathematician who did a lot of work on stochastic processes around the turn of the 20th Century - 1856 to 1922.

--

== !

[plantuml, state-diagram-1, svg, width=600]
----
object A
object B
object C
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
B --> A : 0.2
B -r-> C : 0.8
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
D --> C : 0.5
D --> A : 0.5
----

[NOTE.speaker]
--
Here's a little Markov process.

If we start over here in State A, then the probability of us moving to state B is 0.5, and the probability of moving to state C is 0.25, and to D is 0.25. So, B or C? Let's hear it for B! Let's hear it for C!

Note that the sum of the state transition probabilities is always 1. Even if we can stay where we are, that's still a transition, still gets its own probability.

We're all programmers, the probability that we didn't even
notice that this is a discrete-time Markov chain approaches 1. You can build continuous time Markov processes, but I've no idea how you'd actually do that, and it probably involves some kind of fearsome calculus, so let's pretend I never even mentioned it.

So, we have this little state space, that we're randomly and memorylessly moving around. These have, perhaps surprisingly, a number of real world applications

* Including in macroeconomics - stock price equilibrium, for example, and various Monte Carlo simulations
* Markov chains can be used to model interaction between state actors. We may have to reevaluate that in the light of recent events, but it's been done.
* They have application in algorithmic music generation
* Baseball analytics, apparently (and perhaps not real world)

And, of course, since you've all read the synopsis of this session, you can use Markov chains for algorithmic text generation
--

== !

[.big]#Algorithmic text generation#

[NOTE.speaker]
--
How do we apply Markov chains to text generation?
--

== !

[plantuml, state-diagram-2, svg, width=600]
----
object A
object B
object C
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
B --> A : 0.2
B -r-> C : 0.8
C --> A : 0.4
C --> B : 0.4
C --> C : 0.2
D --> C : 0.5
D --> A : 0.5
----

[NOTE.speaker]
--
Here's the little state transition diagram we had before. Let's strip it right back.
--

== !

[plantuml, state-diagram-stripped, svg, width=600]
----
object A
object B
object C
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
----


== !


[plantuml, state-diagram-3, svg, width=600]
----
object A
A : hello
object B
object C
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25

----

[NOTE.speaker]
--
Well, what if instead of state A was associated with "hello"
--


== !

[plantuml, state-diagram-4, svg, width=600]
----
object A
A : hello
object B
B : there
object C
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
----

[NOTE.speaker]
--
Instead of state B we had "there"
--

== !

[plantuml, state-diagram-5, svg, width=600]
----
object A
A : hello
object B
B : there
object C
C : everybody
object D
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
----

[NOTE.speaker]
--
On state C we had "everybody"
--


== !

[plantuml, state-diagram-6, svg, width=600]
----
object A
A : hello
object B
B : there
object C
C : everybody
object D
D : world
A --> B : 0.5
A --> C : 0.25
A -r-> D : 0.25
----


[NOTE.speaker]
--
And on state D, we have, of course, the Brian Kernighan approved word "world"

So, if we start at "hello", we might transition to, aka generate the next word, "there", or we might generate "world".

Start at "hello". Pick a number. Transition to, aka generate the next world, "world".

Try again, start at "hello". Pick a number. Move to "world"

Try again, start at "hello". Pick a number. Move to "there"

And lo, we algorithmic text generation. I mean, it's not huge, but it's not nothing.

*So that's it right? End of talk.*

I'd buy you all a drink but I'm not sure I can afford it at Bristol prices.

What's the missing step here?
--

== !

[.big]#How do we know what the words are?#


[.big]#How do we know what the probabilities are?#

[NOTE.speaker]
--
How do we know what the probabilities are?
--

== !

[source, javascript]
----
import {make_chain} from "../src/chain.mjs";

const chain = make_chain()

chain.add('hello', 'there')
chain.add('hello', 'everyone')
chain.add('hello', 'there')
chain.add('hello', 'world')
----

[NOTE.speaker]
--
Here's some code.

Within this `chain` object, we've put together a little data structure that looks something like this.
--

== !

[source]
----
hello  ->  (world, 1)
           (there, 2)
           (everyone, 1)
----

[NOTE.speaker]
--
We can see that looks quite similar to our little state transition diagram we had before, except instead of probabilities we've got occurrence counts.
--

== !

[source, javascript]
----
for (let i = 0; i !== 10; ++i)
  console.log("hello " + chain.predict("hello"));
----

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run hello-world

> lets-predict@1.0.0 hello-world
> node bin/predict-hello-world.js

hello everyone
hello there
hello there
hello everyone
hello everyone
hello world
hello everyone
hello world
hello world

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
When we call `predict`, we pluck some magic number, convert those occurrences into probabilities, and lo and behold, out come some greetings.

This is obviously as almost minimal as you can get. We have one starting word, and all the following words are terminal.

We barely even have any code
--

include::chain.adoc[]

== !

[source, javascript]
----
for (let i = 0; i !== 10; ++i)
  console.log("hello " + chain.predict("hello"));
----

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run hello-world

> lets-predict@1.0.0 hello-world
> node bin/predict-hello-world.js

hello everyone
hello there
hello there
hello everyone
hello everyone
hello world
hello everyone
hello world
hello world

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
So, how do we make this a bit more that a simple greetings generator?

We need some more words. We need a Text Corpus
--

== !

[.big]#a Text Corpus#

[NOTE.speaker]
--
Now, in more formal terms, a text corpuses large and systematic. They're gathered using some system, some methodology - for a particlar time period, or geographic location, population, or some combination thereof.

For example the Corpus of London Teenage Speech is a set of samples of spoken English, collected in 1993 from recorded and transcribed conversations by teenagers between the ages of 13 and 17. It's about half a million words, and contains 67 examples of the word 'bum', which I don't think is enough.

Now, I don't have access to any of those academic corpuses, nor did I really just want to go off and download large chunks of the publically available internet.

For my purposes I was rather less formal, and used a text corpus I was able to get full and unconditional permission to use.
--

== !

image::images/toots.png[]

[NOTE.speaker]
--
From myself!

I used my Mastodon archive.
--

[background-iframe="files/toots.html"]
== !


[NOTE.speaker]
--
Here it is. 2384 scintillating lines, every one a certified zinger, containing 46576 individual words.

I've cleaned it up a little to take out other people's handles, and, erm, remove the stronger swearwords -

and looking it now, it already looks like it was generated by some kind of stochastic process. Hmm.

So, how do we build a chain from this?
--

include::code-phrase.adoc[]

== !

[source, javascript]
----
function* tokenise(input) {
  const stream = streamer(input);

  let c = stream.next();
  while (stream.hasNext()) {
    while (isWhitespace(c))
      c = stream.next();

    let token = "";
    for ( ; isAlphanumeric(c); c = stream.next())
      token += c;

    if (token)
      yield token;

    for (; isOtherCharacter(c); c = stream.next())
      yield c;
  }
}
----

[NOTE.speaker]
--
Here we are - this is the tokeniser I put together.

For those of you not familiar, this `function*` means this is a JavaScript generator. If you know Python generators, it's the same but with more curly brackets. It's akin to coroutine - you can yield values, then continue on from there.

`streamer` just puts a little `next`/`hasNext` face on the input, so we can loop over it more easily.

We start by discarding whitespace, then gather up anything that looks like it could be part of a word and yield that, then if there's any punctuation yield that, and round and round until we run out, yielding up a token at a time.

If we put another little layer on top, we can return pairs of words ...
--

=== !

[source, javascript]
----
function streamer(input) {
  let index = 0;
  return {
    next: () => (index !== input.length ? input[index++] : null),
    hasNext: () => index !== input.length,
  };
}
----

== !

[source, javascript]
----
async function* wordPairs(filename) {
  const file = await open(filename);

  let prev = null;
  for await (const line of file.readLines()) {
    for (const token of tokenise(line)) {
      yield [prev, token];
      prev = token;
    }
  }
  yield [prev, null];
}
----

[NOTE.speaker]
--
Does this async/await trouble anyone?

So we've got our `tokenise` generator splitting out toots into tokens, then we've layered another generator around it to pair up those tokens.

We are, essentially running a sliding window over our input stream.

So, we have our pairs of tokens - one at the front, one at the back - are we're just going to build a big list of the tokens in the front, with whatever comes after them and, crucially, how many times each of the following tokens occurs

There's actually quite a lot of code here, because JavaScript's standard library is pretty under powered. In {cpp} you could use a slide_view, in Python you use a couple of iterators and a zip. Got to do it slightly long hand here.
--

== !

[source,javascript]
----
const chain = make_chain();

for await (const [word, follower] of
                  wordPairs('./data/toots.txt'))
  chain.add(word, follower);
----

[NOTE.speaker]
--
We're reading all our pairs of words, we're popping them into the chain, to build up a big list...

So, what does that give us.
--

== !

[code]
----
is      ->  (a, 59)
            (the, 24)
            (',', 20)
            (that, 17)
            ... 207 other tokens ...
            (rarely, 1)

grateful -> (to, 1)
            (!, 1)
            (nation, 1)

...
----

[NOTE.speaker]
--
Our data structure now looks something like this.

And so on, for another 8379 tokens.  8379 seems like a lot, but I can only assume there are a lot of a) names and b) horrific misspellings in there.  There are approximately 35000 state transitions between all those tokens. I tried to get graphviz to draw the diagram, but was still spinning after 10 days.

All this is clear, yes? We can read our source corpus, generating a series of token pairs, producing a big list of each unique token we encounter, together with the counts of the tokens that follow.

If we now take our starter word, "hello", and use that generator our next word, we can take the new word and plug it back in, to get another word, and so on and so on.

And we have recreated our little keyboard game
--

== !

[source,javascript]
----
const tokens = ['hello']

for (let i = 0; i !== 12; ++i) {
  const current_word = tokens[tokens.length - 1];
  const next_word = chain.predict(current_word);
  tokens.push(next_word);
}

console.log(tokens.join(' '));
----

[NOTE.speaker]
--
So that's what we're doing here. We've our starter word, hello, and each time we go round this loop, we use that last word in the array to generate a new word, which we push onto the array.

Then we print it


Would love a pipeline operator here.
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run low-grade-toots

> lets-predict@1.0.0 low-grade-toots
> node bin/predict-from-mastodon.js

hello " & amp ; command . I got them ?
hello " Martian meteorites  I salute you rez in a few do
hello !  I understand from the screenshot � � I have
hello " Kotlin coroutines . You'd also go even seen the transaction management
hello !  I feel to be actually pretty unusual � �
hello , maybe the ability to celebrate . Years ago , despite still
hello " You might wander . I was , even  I was
hello " A kerfuffle overhead , I was a font size  I
hello ! What kind of confusions and clean .  I sort of
hello " up to contribute , but everyone was wrong Love to it

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
And we get this nonsense.

We know this is just a random spray of words, but there are little phrases and things that look almost plausible in there.

What else do you notice about it?

Lots of punctuation after hello. Turns out hello is a really bad starter word for this particular corpus. I've talked a lot on Mastodon about the code I was writing, and about this talk. And my favourite word was "hello"
--

[background-iframe="files/hello.html"]
== !

[NOTE.speaker]
--
And my favourite example word was "hello". I am, it seems, nothing if not predictable

This is interesting, right? This, I'd suggest, is unexpected because hello seems like a perfectly normal word, but in this corpus, it has a certain quirk, a certain characteristic.

Because, our little sliding of word pairs, our occurrence counts have captured something about this body of text.
--

== !

[.big]#A model#

[NOTE.speaker]
--
We have, in fact, built a model.

A model of the particular dialect of English I speak on Mastodon.
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run low-value-toots

> lets-predict@1.0.0 low-value-toots
> node bin/lines-from.js ./data/toots.txt the

the next question answered and peck edits her hall rug on back of

the plus keeper kit , they'll kick up a big you weren't plugged

the building that got my letter . I pay very useful thing going

the workshop ? Some candidates say that work , which I don't need

the first time in the spec in danger , what a cute little

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
Here's the same code, the same text rerun with a different seed word - 'the'.  Now, we'd expect this to give some quite different results, because of the nature of the word we've chosen.

What if we start with a different body of text?
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run oh-mr-darcy

> lets-predict@1.0.0 oh-mr-darcy
> node bin/lines-from.js ./data/pride-and-prejudice.txt the

the valley , cheerful prognostics of Derbyshire . " for
difference in censuring

the ladies are always laughed heartily sorry that it all to
them at

the parish , than from her brother's entrance of my dear .
Elizabeth

the place if she added lustre to understand that their
respective houses for

the particulars , she thought , if she did everything in the
pleasure

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
I still haven't gone off and downloaded large chunks of the public internet. Here, I've used the text of Pride and Prejudice. I've never read it, but my computer has read it for me, and produced these little summaries.  I think I've got the gist.

Pride and Prejudice was published in 1813, so the word hello doesn't appear in the text, but the works splendidly.

Very different feel, right? Pride and Prejudice has about three times as many words as my Mastodon archive, so this is a much bigger model. I don't know necessarily it's more complex - whether Austen's vocabulary is bigger than mine, or her sentence structure more complex, or whatever.

What am I doing? Comparing myself to Jane Austen!

But, could we?

If we had an unknown body of text, could we use our models to say "this is more like Jez's internet spoutings" or "this is more like one of the greatest novelists in the English language"?
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run commonplace

> lets-predict@1.0.0 commonplace
> node bin/lines-from.js ./data/complete-sherlock-holmes.txt the

the way by both stared at Lausanne . My mind had been the

the sight and my skill . He waved his brown crumbly band .

the maid , without omitting anything of a small that of the
certainty

the room , and explained , I clutched her . Mr Holmes ,

the least we have your little chamber . It would seem to puzzle

jez@coldsplash ~/w/j/lets-predict (main)>
----


[NOTE.speaker]
--
Here's some samples from a third corpus - the text of all the Sherlock Holmes novels and stories.

Again, very different feel, from the previous two, but I think it's not as distinctive.

We've taken another big step up in word count - up to 650 thousand words - five times as many as Pride and Prejudice and getting on for 15 times as many words as in my toot archive. But we've also changed another dimension.

My toots, and Pride and Prejudice represent quite a compressed time frame - everything was written over the course of a few months.

A Study in Scarlet was first published in 1887, while the last Holmes story (or at least the last by Conan Doyle), The Adventure of the Retired Colourman, was published in 1926. So these stories cover a span of 40 years. English usage would have morphed and evolvedin that time, and we know that Conan Doyle's attitude to writing had changed. He tried to pack in Holmes at least twice - that's what the fight with Moriaty at the Reichenbach Falls in The Final Problem was trying to. (Didn't work - subsequently wrote another 30 short stories and 2 novels)

So all these factors - the size, the time span, maybe even the different settings of the stories (we think about Holmes being set in London, but he's forever on the train to the West Country, or wherever) - perhaps make this model less distinctive.

How could we help it? How could we make our model better?

To give you a little bit of context, we're into the 4th quarter of the match now. Time is tight, so nothing too ambitious.
--

== !

[.big]#Is this the code +
phrase to activate +
a sleeper agent?#

[NOTE.speaker]
--
Probably the easiest thing we can do is improve our tokenisation. As I said before it's slightly more involved than splitting on whitespace, but only just.

But we know this is a sentence, written in English.

English has a grammar - I mean we might not be able to write it down in EBNF, but there are generally agreed ways in which we use the language.

We know, for example, that this question mark indicates the end of a sentence. What if we just highlight that?
--

== !

[.big]#pass:[<span style="color: yellow"><b>&#x2402;</b></span>]Is this the code +
phrase to activate +
a sleeper agent?pass:[<span style="color: yellow"><b>&#x2403;</b></span>]#

[NOTE.speaker]
--
This is I came up with on the spur of the moment

We pop a special little token in at the start of a sentence, and then a special little token at the of the line.

So, as we run our sliding window across the text, we're annotating with a little bit of extra information. We're saying some of these words occur at the start of a line, and some of these things occur at the end of a line. We're also saying an end is followed by a start.

It's a tiny little thing, right?

But it actually makes a pretty big change. We don't a seed word anymore - we can use the Start of Line token to kick off our generation. Once we encounter an End of Line, we can stop, because we know we've generated a sentence.
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run mid-value-toots

> lets-predict@1.0.0 mid-value-toots
> node bin/autotoot.js

I carried a good: am deeply satisfying, 12 years in Swindon”
hard to be simpler

_Note doing so without getting the grass “A kerfuffle
overhead, one of the UK, and I worked on the 30.

Team sport's great smell of 'moving in NY is it is the modern
with it back in at about 4 different.

It's from HMRC didn't touch you did that baldy?

8 minutes that one of the place I don't use FeedReader on a
lot of C + would think, without an organisation almost all
from.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
I've done a couple of other little things here - when I encounter speech marks I'm just converting those to opening and closing speech marks. That's obviously an extra little helper in the output we generate, but the main reason for doing it was make it easily to format nicely.

Here, rather than just jamming things together, I'm using a bit more of that English knowledge to be a bit smarter about adding spaces or not.  That's not affecting the chain, but it makes the output look more plausible, right?

Because this is a toot, I've only generated a single sentence, but let's apply these changes to our Jane Austen
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run austenesque

> lets-predict@1.0.0 austenesque
> node bin/automeme.js ./data/pride-and-prejudice.txt

 Choose properly overjoyed on that if he had now necessary to
whisper to.  She highly favourable answer.  She concluded her
eyes were ready to prevail on this was least importance could
be entirely, wisest and in machine - past occurrences had been
heard little beyond the bounds of possibilities, and prepared
for Mr Darcy, made; and happy day to Saturday?  Darcy would be
disregarded?  Her daughters, seemed to whisper; coaxed and Mr
Collins, had been distinguished by reflection, when the
respect your house.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
A single sentence is ok for a toot, but Jane Austen was a novelist, so here I'm generating a few sentences, so we can really get the feel.

And, it kind of does. We've got Mr Darcy in there, we've got questions, I mean look at those semi-colons. She loves a subclause, that Jane Austen.

So putting in these little markers has, I think, quite significantly changed, maybe improved, our output generation.
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run holmesian

> lets-predict@1.0.0 holmesian
> node bin/automeme.js ./data/complete-sherlock-holmes.txt

 Then we don't know what had marked the rigour of his pocket of
local powers,” Tropical by an ancient crown which I was
certainly, and five.  You monster!  “Yes.  “What is steep my
person of his agitation that sorry if the hands up to make one
in turn had disguised hand Holmes stretched out my arm.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
Here's the Conan Doyle. I'm still not quite sure we'd immediately tag this as derived from Holmes, but it is showing more character I think.

Obviously I ran this a few times, and this is typical. Much more direct speech than the Austen, much shorter sentences.

That little bit of additional structure is revealing, exposing more of the underlying corpus.

What else might we do?

We are moving, by the way, into the largely theoretical portion of the talk. We're deep into the penalty shoot-out now.
--

== !

[.big]#"My dear Mr Bennet," said his lady to him one day, "have you heard that Netherfield Park is let at last?"#

[NOTE.speaker]
--
We've seen the effect of adding just a little bit of additional information into our Markov Chain. We could extend that to include speech, and perhaps also other punctuation - parenthesis, commas & semi-colons.

You could do that relatively simply, I think. You could put start and end speech markers into the chain in the same way as sentence start and end.

When you generate output, you could add an extra kind of meta-state. We're in speech, let's keep going until we hit the end of speech. But you could be a bit smart about that - you don't just have to wait for closing quotes to come out of your random process, you could pick up on commas and full stops too. Again we're exploiting our knowledge of English to improve the output.

Something that occurred to me while writing this is that speech is quite different from prose. Maybe you could have two chains - one to model the prose, one to model speech. When you're building your models - some people might says training you models - you would switch from one to the other as you encounter speech marks. Similarly, you'd the same when you were generating your output. I think that might be quite interesting, and relatively straightforward.
--

== !

[.big]#He was quiet in his ways, and his habits were regular.#

[NOTE.speaker]
--
We could lean even harder into the structure of our language, perhaps by tagging parts of speech.

Pronoun, verb, adjective, preposition, pronoun, noun, conjunction, pronoun, noun, verb, adjective.

If we had information like this available, we could ensure our algorithmic generated output lined up all its pronouns, for instance. If it started with 'we', then we want 'our' later in the sentence

"We were quiet in our ways, and our habits were regular."

We can probably think of ways we could use parts of speech in our text generation.

Make it more poetic, we stuff some more adjectives in. Perhaps if we want it more robotic, we just drop adjectives entirely.

Early POS taggers were rules-based. A chap called Eric Brill pioneered this approach the early 90s. I worked on POS tagging in around 2000, and I think I used his code then. More recent parts of speech taggers, use hidden Markov models.
--

== !

[.big]#Tried explaining that I'm a knowledge worker and thus unqualified to "shift the bags of rubble",#

[NOTE.speaker]
--
We could stem, especially if that's supported by parts of speech.

Stemming means reducing to the "stem" - from explaining to explain, worker to work, that kind of thing.

I mentioned making sure out pronouns align in a sentence. If we word stem, as we produce our output we can ensure that our verb cases match up, that our tenses match up.

If we want something instructional, we could generate particular verb forms - you must place the biscuit in the basket rather than you may wish to place the biscuit in the basket.

Again, some stemming approaches are stochastic. Again, this language dependent - English is quite easy, Arabic very hard.
--

== !

[.big]#but Natalie just said "yea, yea, expand your CV and shift them"#

[NOTE.speaker]
--
We can also continue to train it. Building the model needn't be a one-off exercise like I've been showing here. We can continue to add new entries into our chain - this is what our keyboards are doing as we type, so the word lists and the probabilities change over time.

We could reinforce the model - say yes, we like this output more than we like that output. This is what we're when we select a suggestion that our keyboard gives us - we're tweaking the weights, nudging the probabilities to tilt things a little more towards the word we've chosen.

We could add a time element, either by preferring newing additions to our model, or by aging out older entries. Or both. After all, if Elizabeth didn't marry Mr Darcy, she'd probably her devices to stop sticking his name in front of her, right?
--

== !

[.big]#I looked back on +
the long chain of curious +
circumstances#

[NOTE.speaker]
--
I think we could do those things without changing the Markov chain code I showed earlier. Preprocessing and annotating tokens on the way in, yes, postprocessing on the way out, yes, but the bit in the middle would stay the same.

If we're prepared to step away from that, all kinds of other possibilities open up - latent semantic analytics, word2vec, all kinds of exciting things.

Latent semantic analysis is the idea the words with related meanings are clustered together in texts.

word2vec is technique capturing meaning of words based on their surrounding context.

We're moving very much into this takes more time that is available in a season of hockey, let alone just a the one game, but NLP is a fascinating topic.

Before we finish, let's have a look at a couple of things we can do with the code as is.

If we had some nice Austen, and some ok Holmes, and well, my toots, and we put them all together, in to one great big corpus?

What would that look like?
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run chimera

> lets-predict@1.0.0 chimera
> node bin/chimera.js

 If the dwarfish chaparral bushes.  I can see her brother's
indifference while? ” If you mean _nobody_ needs Yes, then all
his large party to answer this means of anything at work in
which lead me. ” said, and Drebber passed under the good to
the track, if he had better.  Just except in the credit of
course, and cycling.  Just listened with him is not any but
I'm not so? ” Miss Elizabeth began to his fault the letter of
understand anything ridiculous elections.  Always I'd actually
ship anything holiday?  Not as sensitive to the front garden.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
It's a mess. It's not really anything.

More is definitely not better here.

Remember my toots by themselves had 35000 state transitions. We're up to who knows what now. These Markov models are simple, but complex at the same time.

I mentioned before that some tokens have very small number of transitions, while others, like a, the, commas, and what-not, have this very large number. As the size of the corpus increases, and the variation within that corpus - we're combining now three very different text bodies - the fan-out from, say, a comma just gets bigger and bigger and bigger, and so, I don't know, the coherence of the output starts to fall off.

We get these little islands of stability, we can see them here - "dwarfish chapparal bushes" - but then we hit the end of the sentence and, it's all up in the air again.

Quality here is very subjective, but I would say this is not good.
--

include::secret-agent.adoc[]

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run chimera 2

> lets-predict@1.0.0 chimera
> node bin/chimera.js 2

But with such furniture as you have now made it second nature.
They thought to myself,” said Darcy, a dangerous route.  I
wonder how he came back at home, Elizabeth felt an anxiety on
the JVM.  Without meaning to hurt either of them.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
What do we make of this? Better? Quite a lot better?

It certainly reads more naturally. There are some actual proper sentences in there, although I think we can still see it moving from Austen to Mastodon to Conan Doyle.

If widening that window to 2, is better, what if we go to 3 or 4?
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run chimera 4

> lets-predict@1.0.0 chimera
> node bin/chimera.js 4

  We've seen the real world consequences before. “” There are
no crimes and no criminals in these days, “he said;” I know
nothing more of him than I have learned from him occasionally
in the laboratory.  Jane met her with a smile of such sweet
complacency, a glow of such happy expression, as sufficiently
marked how well she was satisfied with the occurrences of the
evening.  Her curiosity, however, was unexpectedly relieved.

jez@coldsplash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
Well now, this looks great.

But hang on. "I know nothing more of him that I have learned from meeting him occasionally in the laboratory"? That's a line straight of out The Sign Of The Four, when Stamford is about to introduce Watson to Holmes for the first time.

"Jane met here with a smile of such sweet complacency...", that must be Austen.

Our model is now just regurgitating the sources texts more or less wholesale. As we've widened the window, increased the length of the tuple of words, the number of possible following words has more or less collapsed.

A width of two, that was ok - it brought a certain amout of stability while still allowing variation.  "nothing more" occurs nearly 40 times in Holmes stories - nothing more I could do, nothing more to say, nothing more substantial, nothing more exciting, and so on - so there's still room to manoeuvre.

But when we turn that up a little too much, suddenly we're revealing our training data to the world.
--

== !

[source, shell]
----
jez@coldsplash ~/w/j/lets-predict (main)> npm run mid-value-toots 2

> lets-predict@1.0.0 mid-value-toots
> node bin/autotoot.js 2

My phone keyboard asked me about 4 ads, I guess.

You'd expect that to take the car stiffness.

This is an organisation almost entirely built around public
image (But also yes, that means it should be directing the
actions of a territorial barney.

Hockey opposition have sacked off.

Amanda Grant's The Joy Of Vegan Cooking is, using two
different dates formats in a very different vein The Amazing
Remarkable Monsieur Leotard are all worth your time.

jez@coldspash ~/w/j/lets-predict (main)>
----

[NOTE.speaker]
--
Turns out, with a less homogeneous data set, you can leak training data even with a window of 2.

Here's my mastodon corpus again, with a window of two.

I'd say, for randomly generated sentences these are pretty good, but "Hockey opposition have sacked off." is a verbatim regurgitation of a bit of on internet ranting. The second half of that last line "a very different vein The Amazing Remarkable Monsieur Leotard are all worth your time" is also straight out of the corpus. "in a" is the key part there, the occurs a lot, but then once we've headed down "a very" (which occurs 6 times) and then "very different" (once) we're locked into the rest of that sentence.
--

== !

[.big]#Even small systems get the blues#

[NOTE.speaker]
--
On the one hand, this is a just a bit of fun. Programming for money is great and all, but it's often quite run of the mill, so if you need to, just bunk off for a couple of hours (pop some sport on in the background if that's your thing, play some Ennio Morricone, whatever works for you) - noone will notice - and write something fun, maybe something a bit magic. You'll feel better for it.

On the other hand, the title of this talk is obviously an allusion to the AI hype/future wonderland/torment nexus that's swirling and whirling all around us.

Even a tiny system like this can display surprising, maybe even convincing, behaviour. We've got about a hundred lines of code here, but the model we've built with it is literally beyond comprehension. As we feed it more and more text, as the complexity increases, it actually degrades, it gets worse. As we widened the window, it got better and then collapsed. Larger, more inherently complex, systems will display even more surprising and convincing behaviour, but as they grab more and more text, it looks like they too will actually get worse and worse.

Will it be quite as obvious as what we've just seen? Well, it's not unknown for chatbots to go full-on Nazi within hours or days, but the more subtle degradations? Will we notice? And if we don't, does that mean they're ok, or that we just haven't spotted the problems yet?

I've just said programming can be a ton of fun, because it can. We can reach out and explore these amazing problems, and for every problem there's an infinity of solutions. There's this incredible multidimensional space we can shape and travel through, so it can be fun, but it can be dangerous.

There are consequences from the things we build too.

So, have fun, but let's be careful out there.

Thank you all for your time and attention
--
